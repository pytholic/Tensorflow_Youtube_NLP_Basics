{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d5ba3a0",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "## What is Tokenization?\n",
    "Representing words in a way that a computer can process them is called **Tokenization**. Different words can have same words just in different order. This makes it hard for us to understand the sentiment of a word just by the letetrs in it. So it might be easier instead of encoding letetrs to encode words.\n",
    "\n",
    "## Tokenization in Tensorflow\n",
    "Tensorflow has a **Tokenizer** API for tokenization. \n",
    "\n",
    "* We can create an instance of tokenizer object. The `num_words` paramter is the max number of words to keep. Imagine if we have hundreds of books to tokenize, but we just want the hundred most frequent words in all of that. This paramter i.e. `num_words` will automatically do that for us when we do the next step.\n",
    "* Now we tell the `tokenizer` to fo through all the text and then fit itself to them\n",
    "* The full list of words is available as the tokenizer's `word_index` property. We can print it out\n",
    "\n",
    "**Note:** \n",
    "The tokenizer is also smart enough to catch some exceptions. Look at how **\"dog!\"** has same token as **\"dog\"**. The nice thing is that tokenizer is smart enough to recognize this and not create a new token.\n",
    "\n",
    "\n",
    "# Turning sequences into data\n",
    "We will add another sentence to out set of text. We are doing this because existing sentences all have four words, and it is important to see how to manage sentences or sequences of different lengths.\n",
    "\n",
    "The tokenizer supports a method called `text_to_sequences`, which perform most of the work for you. It creates sequences sequences of tokens representing each sentence.\n",
    "\n",
    "## Handling unseen words\n",
    "Now, we have the basic tokenization done, but there is a catch. This is all very well for getting data ready for training a neural network, but what happens when that neural network needs to classify text and there are wprds in the text that it has never seen before?\n",
    "\n",
    "We have used a set of sentences for training a neural network. The tokenizer gets `word_index` from these and creates a sequence for us. So now if we want to sequence sentences containing new unseen words that were not present in our initial set of data and hence not present in the `word_index`, what's going to happen?\n",
    "\n",
    "-> A five word sentence will end up as four word sequence, because new word was not present in the `word_index`.\n",
    "\n",
    "In order to not lose the length of the sequences, we can use a little trick. By using the `OOV` token property and setting it to soemthing that we would not expect to see in the corpus, the tokenizer will create a token for that. Then it will replace words that it does not recognize with the **Out of Vocabulary (OOV)** token instead. It is simple but effective. Meaning may not be correct, but atleast the length is preserved.\n",
    "\n",
    "## How to handle sentences of different lengths\n",
    "`Images` are usually the same size, so we can train a neural network easily. How do we solves this for text data?\n",
    "\n",
    "The advanced answer is to use something called `Ragged Tensor`, but that is out of scope for now. We will use a simpler and different solution called `padding`.\n",
    "\n",
    "* Import `pad_sequences` from preprocessing. Now just pass the sequences to this function and rest if done automatically for you.\n",
    "\n",
    "**Note:** \n",
    "* Our longest sentence has seven words, so pad_sequence will measure that and pad remaining sentence with zeros in the front to make the length same.\n",
    "* **OOV** isn't '0', it is '1'. '0' means padding\n",
    "* If you want zeros after the sentence, set `padding` parameter in `pad_sequences` to `post`\n",
    "* You can also specify `maxlen` parameter to specify the desired length of the padded sequences\n",
    "\n",
    "What if sentences are longer than the specified max lenght? Then you can use `truncating` parameter to specify how to truncate by chopping off words either from the end or from the beginning.\n",
    "\n",
    "\n",
    "# Training a model to recognize sentiment\n",
    "## Dataset\n",
    "\n",
    "We use Rishabh Mirza's [dataset](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection) from kaggle. The dataset is nice and simple.\n",
    "\n",
    "* The `is_sarcastic` filed is 1 if the record is sarcastic, otherwise 0\n",
    "* There is a `headline` of the news article that we will train on\n",
    "* There is also an `article_link` to the original news article\n",
    "\n",
    "### Format\n",
    "The data is stored in `json` format. We will have to convert it to `python` format for training. Every json element will become a python list element.\n",
    "\n",
    "Python has a `json-toolkit` that can achieve this!\n",
    "\n",
    "## Preprocessing\n",
    "* Create `token` for every word in the corpus\n",
    "* Convert sentences into `sequences` of tokens and `pad` then to the same length\n",
    "* Slice the data into training and test set\n",
    "\n",
    "**Note:** We need to make sure that our `tokenizer` just fits the training data and not test data. Use `fits_on_texts` only on training data.\n",
    "\n",
    "## Embeddings\n",
    "`Word embedding`, `word vector` or `embeddings` is a term used for the representation of words for text analysis, typically in the form of a `real-valued vector` that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings allow words with similar meaning to have a similar representation.\n",
    "\n",
    "In the code, the top layer is an `Embedding layer`.\n",
    "\n",
    "## Train the model\n",
    "Use `model.fit` to train the network.\n",
    "\n",
    "## Test\n",
    "Convert the new sentence into sequences using `tokenizer`, `pad` them with the same padding type as training data and the use `model.predict`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d35b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
